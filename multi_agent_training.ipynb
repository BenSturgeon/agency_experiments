{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "import torch as T\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque , namedtuple\n",
    "from itertools import count\n",
    "# set up matplotlib\n",
    "from IPython import display\n",
    "\n",
    "from feature_extractor import MinigridFeaturesExtractor\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class replay_memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(input_dims, 128) # First hidden layer\n",
    "        self.fc2 = nn.Linear(128, 128)        # Second hidden layer\n",
    "        self.fc3 = nn.Linear(128, n_actions)  # Output layer\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        action_probs = F.softmax(self.fc3(x), dim=1)\n",
    "        return action_probs\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, input_dims, n_actions,  device, env, label, LR=0.001, gamma=0.99, eps_start=0.9, eps_end=0.05, eps_decay=1000, tau=0.005):\n",
    "        self.policy_network = PolicyNetwork(input_dims, n_actions).to(device)\n",
    "        self.target_network = PolicyNetwork(input_dims, n_actions).to(device)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        self.memory = replay_memory(100000)\n",
    "        self.steps_done = 0\n",
    "        self.gamma = gamma\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.tau = tau\n",
    "        self.LR = LR\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=self.LR, amsgrad=True)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = 32\n",
    "        self.env = env\n",
    "        self.label = label\n",
    "        self.current_state = None\n",
    "        self.chkpt_file = \"checkpoint_minigrid_\" + str(self.label) + \".pth\"\n",
    "        self.loss_record = []\n",
    "        self.feature_extractor = MinigridFeaturesExtractor(env.observation_space[self.label], features_dim=64)\n",
    "\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.policy_network.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.policy_network.load_state_dict(T.load(self.chkpt_file))\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with T.no_grad():\n",
    "                # if not isinstance(state, torch.Tensor) or state.shape[1] != 64:\n",
    "                #     state = self.feature_extractor(state).view(-1, 64)\n",
    "                # print(state.shape)\n",
    "                return self.policy_network(state).to(self.device).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return T.tensor([self.env.action_space[self.label].sample()], device=self.device, dtype=T.long)\n",
    "\n",
    "    def process_step(self, action, next_state, reward, done):\n",
    "\n",
    "\n",
    "\n",
    "        action_tensor = torch.tensor([action], device=self.device, dtype=torch.long)\n",
    "        reward_tensor = torch.tensor([reward], device=self.device, dtype=torch.float)\n",
    "\n",
    "\n",
    "        self.memory.push(self.current_state, action_tensor, next_state, reward_tensor)\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.optimize_model()\n",
    "\n",
    "        if done:\n",
    "            self.current_state = None\n",
    "\n",
    "        self.soft_update_target_network()\n",
    "\n",
    "    def soft_update_target_network(self):\n",
    "        for target_param, param in zip(self.target_network.parameters(), self.policy_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = T.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=T.bool)\n",
    "        non_final_next_states = T.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = T.cat(batch.state)\n",
    "        action_batch = T.cat(batch.action)\n",
    "        reward_batch = T.cat(batch.reward)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_network\n",
    "        state_action_values = []\n",
    "        for agent_state in state_batch:\n",
    "            if agent_state.dim() == 1:\n",
    "                agent_state = agent_state.unsqueeze(0)\n",
    "            print(agent_state.shape, action_batch.shape)\n",
    "            agent_state_action_values = self.policy_network(agent_state).gather(1, action_batch.unsqueeze(0))\n",
    "            state_action_values.append(agent_state_action_values)\n",
    "        state_action_values = T.cat(state_action_values)\n",
    "\n",
    "        # state_action_values = self.policy_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_network; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = T.zeros(self.batch_size, device=device)\n",
    "        with T.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0]\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.loss_record.append(loss)\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        # In-place gradient clipping\n",
    "        T.nn.utils.clip_grad_value_(self.policy_network.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "class MultiAgentSystem:\n",
    "    def __init__(self, env, num_agents, device):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = [Agent(input_dims=64, n_actions=env.action_space[0].n, device=self.device, env=env, label=i) for i in range(num_agents)]\n",
    "        self.episode_durations = []\n",
    "        self.loss_record = []\n",
    "        self.episode_rewards = []\n",
    "        self.max_score = -math.inf\n",
    "        self.min_score = math.inf\n",
    "        self.avg_score = 0\n",
    "\n",
    "\n",
    "\n",
    "    def collect_actions(self):\n",
    "        actions = {}\n",
    "        states = [agent.current_state for agent in self.agents]\n",
    "        for agent_id, (agent, state) in enumerate(zip(self.agents, states)):\n",
    "            action = agent.select_action(state)\n",
    "            actions[agent_id] = action\n",
    "        return actions\n",
    "\n",
    "    def process_observations(self, observation,agent_label):\n",
    "        observation_tensor = torch.tensor(observation[agent_label], device=self.device, dtype=torch.float).unsqueeze(0)\n",
    "        feature_vector = self.agents[agent_label].feature_extractor(observation_tensor).view(-1, 64)\n",
    "        return feature_vector\n",
    "\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        pbar = tqdm(range(1, num_episodes))\n",
    "        for i_episode in pbar:\n",
    "            # Initialize the environment and get its state\n",
    "            observations, info = self.env.reset()\n",
    "\n",
    "            for agent in self.agents:\n",
    "                state_features = self.process_observations(observations, agent.label)\n",
    "                agent.current_state = state_features\n",
    "            accumulated_reward = 0\n",
    "\n",
    "            counter = 0\n",
    "\n",
    "            for t in count():\n",
    "                counter +=1\n",
    "                actions = self.collect_actions()\n",
    "                observations, rewards, terminated, truncated, _ = self.env.step(actions)\n",
    "\n",
    "                for agent in self.agents:\n",
    "                    action = actions[agent.label]\n",
    "                    reward = rewards[agent.label]\n",
    "                    done = terminated[agent.label] or truncated[agent.label]\n",
    "                    state_features = self.process_observations(observations, agent.label)\n",
    "                    next_state = None if terminated[agent.label] else state_features\n",
    "\n",
    "\n",
    "                    agent.process_step(action, next_state, reward, done)\n",
    "\n",
    "                    # After accumulating states and actions we update the state\n",
    "                    agent.current_state = next_state\n",
    "\n",
    "\n",
    "                    print(\"states have changed\", done, counter)\n",
    "\n",
    "                if all(terminated.values()):\n",
    "                    self.episode_rewards.append(accumulated_reward)\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multigrid.envs import EmptyEnv\n",
    "from custom_wrappers import MultiAgentImgObsWrapper\n",
    "\n",
    "env = EmptyEnv(render_mode=\"rgb_array\", agents=2)\n",
    "env = MultiAgentImgObsWrapper(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "controller = MultiAgentSystem(env, 2, device)\n",
    "controller.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multigrid_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
